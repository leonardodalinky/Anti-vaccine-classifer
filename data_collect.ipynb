{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML final project\n",
    "In this project, I will handle the final project of machine learning course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm, trange\n",
    "from typing import List, Dict, Any, Optional\n",
    "from traceback import print_exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collecting\n",
    "At first, we need to collect data as much as I can.\n",
    "\n",
    "`stweet` is the scraper I used to collect twitter streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import stweet as st\n",
    "import timeout_decorator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get through the GFW :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create web client\n",
    "proxy_url = \"http://localhost:8889\"\n",
    "proxy_config = st.http_request.RequestsWebClientProxyConfig(\n",
    "    http_proxy=proxy_url,\n",
    "    https_proxy=proxy_url,\n",
    ")\n",
    "web_client = st.DefaultTwitterWebClientProvider.get_web_client()\n",
    "web_client.proxy = proxy_config\n",
    "\n",
    "WAIT_TIMEOUT = 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block loads tweet-ids from `avax-tweet-ids/` and pull down the valid content into `avax-tweets`. All these data come from [avax-tweets-dataset](https://github.com/gmuric/avax-tweets-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Processing(1/9428): avax-tweet-ids/2020-11/2020-11-16-00.txt\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Tweet ID 1328249501132468225:   2%|▏         | 5/237 [00:16<13:16,  3.43s/it]"
    }
   ],
   "source": [
    "# create data dir\n",
    "Path(\"avax-tweets/\").mkdir(exist_ok=True)\n",
    "# file list\n",
    "glob_list = glob(\"avax-tweet-ids/*/*.txt\")\n",
    "random.shuffle(glob_list)\n",
    "\n",
    "# collect anti-vaccine textual data\n",
    "def load_tweets_id(filepath: Path) -> List[str]:\n",
    "    with filepath.open(\"r\", encoding=\"utf8\") as fp:\n",
    "        tmp = fp.readlines()\n",
    "        ret = list(map(lambda x: x.strip(), tmp))\n",
    "        random.shuffle(ret)\n",
    "        return ret[:min(1000, len(ret))]\n",
    "\n",
    "# scraper\n",
    "@timeout_decorator.timeout(seconds=WAIT_TIMEOUT, use_signals=False)\n",
    "def scrape_tweet_id(tweet_id: str):\n",
    "    output_collect = st.CollectorRawOutput()\n",
    "    id_task = st.TweetsByIdTask(tweet_id)\n",
    "    st.TweetsByIdRunner(id_task, raw_data_outputs=[output_collect], web_client=web_client).run()\n",
    "    return output_collect\n",
    "\n",
    "for idx, path_str in enumerate(glob_list):\n",
    "    print(f\"Processing({idx + 1}/{len(glob_list)}): {path_str}\")\n",
    "    filepath = Path(path_str)\n",
    "    tweet_ids = load_tweets_id(filepath)\n",
    "    output_file_path = Path(\"avax-tweets\").joinpath(f\"{filepath.stem}.txt\")\n",
    "    if output_file_path.exists():\n",
    "        # 已经走过的文件就不走了\n",
    "        continue\n",
    "    is_written = False\n",
    "    fp = output_file_path.open(\"w\", encoding=\"utf8\")\n",
    "    qbar = trange(len(tweet_ids))\n",
    "    for i in qbar:\n",
    "        tweet_id = tweet_ids[i]\n",
    "        qbar.set_description(f\"Tweet ID {tweet_id}\")\n",
    "        try:\n",
    "            output_collect = scrape_tweet_id(tweet_id)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except timeout_decorator.TimeoutError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print_exception(e)\n",
    "            sys.exit(-1)\n",
    "        for raw_data in output_collect.get_raw_list():\n",
    "            json_data = json.loads(raw_data.to_json_line())\n",
    "            legacy_data = json_data[\"raw_value\"][\"legacy\"]\n",
    "            if legacy_data[\"id_str\"] == tweet_id and legacy_data[\"lang\"] == \"en\":\n",
    "                try:\n",
    "                    legacy2_data = json_data[\"raw_value\"][\"legacy\"][\"retweeted_status_result\"][\"result\"][\"legacy\"]\n",
    "                except KeyError:\n",
    "                    legacy2_data = None\n",
    "                if legacy2_data is not None:\n",
    "                    out_str = legacy2_data[\"full_text\"].replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "                else:\n",
    "                    out_str = legacy_data[\"full_text\"].replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "                fp.write(f\"{tweet_id} {out_str}\\n\")\n",
    "                fp.flush()\n",
    "                is_written = True\n",
    "                break\n",
    "    qbar.close()\n",
    "    fp.close()\n",
    "    if not is_written:\n",
    "        output_file_path.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting the anti-vaccine data, we shall collect the normal data from [COVID19_Tweets_Dataset](https://github.com/lopezbec/COVID19_Tweets_Dataset).\n",
    "Almost the same as the above code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Processing(1/7838): covid19-tweet-ids/2021_03/2021_03_06_19_Summary_Sentiment.csv\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Tweet ID 1368282855365836803:  11%|█         | 111/1000 [05:40<55:41,  3.76s/it]"
    }
   ],
   "source": [
    "# create data dir\n",
    "Path(\"covid19-tweets/\").mkdir(exist_ok=True)\n",
    "# file list\n",
    "glob_list = glob(\"covid19-tweet-ids/*/*.csv\")\n",
    "random.shuffle(glob_list)\n",
    "\n",
    "# collect normal covid19 textual data\n",
    "def load_non_negative_tweets_id(filepath: Path) -> List[str]:\n",
    "    df = pd.read_csv(str(filepath))\n",
    "    id_series = df[df[\"Sentiment_Label\"] != \"negative\"][\"Tweet_ID\"].astype(str)\n",
    "    return id_series.sample(n=min(1000, id_series.shape[0])).to_list()\n",
    "\n",
    "# scraper\n",
    "@timeout_decorator.timeout(seconds=WAIT_TIMEOUT, use_signals=False)\n",
    "def scrape_tweet_id(tweet_id: str):\n",
    "    output_collect = st.CollectorRawOutput()\n",
    "    id_task = st.TweetsByIdTask(tweet_id)\n",
    "    st.TweetsByIdRunner(id_task, raw_data_outputs=[output_collect], web_client=web_client).run()\n",
    "    return output_collect\n",
    "\n",
    "for idx, path_str in enumerate(glob_list):\n",
    "    print(f\"Processing({idx + 1}/{len(glob_list)}): {path_str}\")\n",
    "    filepath = Path(path_str)\n",
    "    tweet_ids = load_non_negative_tweets_id(filepath)\n",
    "    output_file_path = Path(\"covid19-tweets\").joinpath(f\"{filepath.stem}.txt\")\n",
    "    if output_file_path.exists():\n",
    "        # 已经走过的文件就不走了\n",
    "        continue\n",
    "    is_written = False\n",
    "    fp = output_file_path.open(\"w\", encoding=\"utf8\")\n",
    "    qbar = trange(len(tweet_ids))\n",
    "    for i in qbar:\n",
    "        tweet_id = tweet_ids[i]\n",
    "        qbar.set_description(f\"Tweet ID {tweet_id}\")\n",
    "        try:\n",
    "            output_collect = scrape_tweet_id(tweet_id)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except timeout_decorator.TimeoutError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print_exception(e)\n",
    "            sys.exit(-1)\n",
    "        for raw_data in output_collect.get_raw_list():\n",
    "            json_data = json.loads(raw_data.to_json_line())\n",
    "            legacy_data = json_data[\"raw_value\"][\"legacy\"]\n",
    "            if legacy_data[\"id_str\"] == tweet_id and legacy_data[\"lang\"] == \"en\":\n",
    "                try:\n",
    "                    legacy2_data = json_data[\"raw_value\"][\"legacy\"][\"retweeted_status_result\"][\"result\"][\"legacy\"]\n",
    "                except KeyError:\n",
    "                    legacy2_data = None\n",
    "                if legacy2_data is not None:\n",
    "                    out_str = legacy2_data[\"full_text\"].replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "                else:\n",
    "                    out_str = legacy_data[\"full_text\"].replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "                fp.write(f\"{tweet_id} {out_str}\\n\")\n",
    "                fp.flush()\n",
    "                is_written = True\n",
    "                break\n",
    "    qbar.close()\n",
    "    fp.close()\n",
    "    if not is_written:\n",
    "        output_file_path.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clean\n",
    "After we collect enough data, we have to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the anti-vaccine tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 50.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import text_clean\n",
    "\n",
    "# create new dir to store the clean data\n",
    "clean_dir = Path(\"avax-tweets-clean/\")\n",
    "clean_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for path_str in tqdm(glob(\"avax-tweets/*.txt\")):\n",
    "    old_path = Path(path_str)\n",
    "    new_path = clean_dir.joinpath(old_path.name)\n",
    "    text_clean.preprocess_tweet_file(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the normal covid19 tweets data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import text_clean\n",
    "\n",
    "# create new dir to store the clean data\n",
    "clean_dir = Path(\"covid19-tweets-clean/\")\n",
    "clean_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for path_str in tqdm(glob(\"covid19-tweets/*.txt\")):\n",
    "    old_path = Path(path_str)\n",
    "    new_path = clean_dir.joinpath(old_path.name)\n",
    "    text_clean.preprocess_tweet_file(old_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}